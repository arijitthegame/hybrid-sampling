{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from data_utils import * \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A simple LSTM or bilSTM model\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.2):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp) # Token2Embeddings\n",
    "       \n",
    "        self.rnn = nn.LSTM(ninp, ninp, nlayers, dropout=dropout)\n",
    "        \n",
    "        self.decoder = nn.Linear(nhid, ntoken, bias=False)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "    \n",
    "        self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        \n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "       \n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded, hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "       \n",
    "        return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n",
    "                    Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 32\n",
    "eval_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data from ptb and add in your path\n",
    "corpus_raw = Corpus('/Users/arijitsehanobish/simple-examples/ptb_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = batchify(corpus_raw.train, train_batch_size) # size(total_len//bsz, bsz)\n",
    "val_data = batchify(corpus_raw.valid, eval_batch_size)\n",
    "test_data = batchify(corpus_raw.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 200 # interval to report\n",
    "ntokens = len(corpus_raw.dictionary)\n",
    "\n",
    "# choose bidirectional vs unidirectional model and other model hyperparameters\n",
    "directions = 1\n",
    "hidden_size = 200\n",
    "\n",
    "n_layers = 2\n",
    "net = RNNModel(ntokens, hidden_size, hidden_size, n_layers, dropout=.2)\n",
    "bptt = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download from drive and put the right path\n",
    "net.load_state_dict(torch.load('/Users/arijitsehanobish/simple-examples/models/lstm_tied_warmstart_try4.pkl'))\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need 3 things: 1) get true softmax scores via the function (def get_true_softmax_scores(data_source, net, bptt)), 2) class embeddings (ntokens, hidden_size) (def get_class_embeddings(net)), 3) output of the tokens (def get_model_embeddings(data_source, net, bptt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "net is the model. data_source is the test_data, bptt=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
